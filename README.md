# Проектный практикум 3. Учебная задача


**Команда №1**
- Алексеева А.А.
- Осипов Н.В.
- Сокирка А.В.
- Султанов Э.М.
- Шибакова А.А.

## Генерация отзывов с использованием нейронной сети

### Описание проекта

Целью проекта является создание нейронной сети, способной генерировать текстовые отзывы о различных местах на основе входных параметров, таких как категория места, средний рейтинг и ключевые слова. Для этого был проведен анализ и подготовка данных, а также расширение датасета новыми признаками.

Данные для проекта взяты из открытого датасета отзывов на организации в России, доступного на [GitHub](https://github.com/yandex/geo-reviews-dataset-2023). Датасет содержит 500 000 уникальных отзывов, опубликованных на Яндекс Картах с января по июль 2023 года.

---
Данные: https://github.com/yandex/geo-reviews-dataset-2023


### Состав датасета

Датасет содержит следующие столбцы:
- **address**: Адрес организации.
- **name_ru**: Название организации.
- **rubrics**: Список рубрик, к которым относится организация.
- **rating**: Оценка пользователя (от 0 до 5).
- **text**: Текст отзыва.

---

### Шаги выполнения работы

#### 1. Импорт библиотек
Для выполнения анализа и обработки данных были использованы следующие библиотеки:
- `pandas` для работы с данными.
- `matplotlib` и `seaborn` для визуализации.
- `nltk` и `pymorphy2` для обработки текста.
- `sklearn` для кластеризации и анализа TF-IDF.
- `geopy` для геокодирования адресов.

#### 2. Загрузка и первичная обработка данных
- Датасет был загружен в формате `.parquet`.
- Проведена очистка данных:
  - Удалены строки с пропущенными значениями.
  - Удалены дубликаты текстов отзывов.
- Вычислена базовая статистика:
  - Количество уникальных организаций, адресов и рубрик.  
  - Построены графики распределения уникальных объектов (организаций, адресов, рубрик).
  [img](https://github.com/sultanovemil/PP_3_URFU/blob/main/img/destribution.png)
	

#### 3. Геокодирование адресов
- Попытка извлечь координаты (широта и долгота) с использованием библиотеки `geopy` и сервиса Nominatim.
- Из-за ограничений API и длительного времени загрузки данных через Docker, геокодирование было отложено.

#### 4. Расширение данных
##### 4.1. Лемматизация текста
- Для обработки текстов отзывов была реализована функция лемматизации с использованием библиотеки `pymorphy2`.
- Удалены стоп-слова, добавлены дополнительные стоп-слова для русского языка.

##### 4.2. Выделение ключевых слов
- С помощью TF-IDF анализа были выделены ключевые слова для каждой рубрики.
- Датасет был дополнен новым признаком `kw_by_rub` — ключевые слова, соответствующие рубрикам организации.

[img](https://github.com/sultanovemil/PP_3_URFU/blob/main/img/key_words.png)

##### 4.3. Замена рейтинга
- Из-за дисбаланса в распределении рейтингов (преобладание оценок "5") рейтинг был заменен на средний рейтинг организации.
- Построен график распределения отзывов по рейтингу.
[img](https://github.com/sultanovemil/PP_3_URFU/blob/main/img/Rating_destribution.png)

#### 5. Очистка текстов отзывов
- Проведена базовая очистка текстов:
  - Приведение текста к нижнему регистру.
  - Удаление HTML-тегов, лишних символов, цифр и лишних пробелов.
  - Замена нескольких точек на одну.

---

### Итоговый датасет

После обработки и расширения данных датасет содержит следующие столбцы:
- **address**: Адрес организации.
- **name_ru**: Название организации.
- **rubrics**: Список рубрик.
- **rating**: Средний рейтинг организации.
- **text**: Очищенный текст отзыва.
- **aspects**: Лемматизированный текст отзыва с выделением существительных.
- **kw_by_rub**: Ключевые слова, соответствующие рубрикам организации.

---

### Результаты

1. Датасет был очищен от пропусков и дубликатов.
2. Добавлены новые признаки:
   - Лемматизированный текст отзывов.
   - Ключевые слова для каждой рубрики.
   - Средний рейтинг организации.
3. Проведена базовая визуализация данных.
4. Данные подготовлены для обучения нейронной сети.

---

### Вторая часть проекта: Обучение нейронной сети

Во второй части проекта была реализована дообученная нейронная сеть на основе модели GPT-2 `sberbank-ai/rugpt3small_based_on_gpt2`, способная генерировать текстовые отзывы о различных местах. Для обучения использовались данные, подготовленные на первом этапе проекта, включающие очищенные тексты отзывов, категории мест, средние рейтинги и ключевые слова.

---

### Шаги выполнения работы

#### 1. Импорт библиотек
Для выполнения задачи использовались следующие библиотеки:
- `pandas` для работы с данными.
- `transformers` для работы с предобученными моделями и токенизаторами.
- `datasets` для преобразования данных в формат, совместимый с библиотекой `transformers`.

#### 2. Загрузка и подготовка данных
- Данные были подготовленного на первом этапе проекта.
- Проведена проверка на наличие пустых значений в столбце `text`. Строки с пустыми значениями были удалены.
- Для обучения модели текст отзывов был преобразован в формат, включающий:
  - Категорию места.
  - Средний рейтинг.
  - Ключевые слова.
  - Текст отзыва.

- Данные были преобразованы в формат `Dataset` из библиотеки `datasets` для дальнейшей токенизации.

#### 3. Токенизация данных
- Для токенизации использовалась предобученная модель `sberbank-ai/rugpt3small_based_on_gpt2`.
- Тексты были токенизированы с учетом следующих параметров:
  - Максимальная длина текста: 512 токенов.
  - Заполнение до максимальной длины.
  - Добавление меток (`labels`), совпадающих с токенизированным входом (`input_ids`), для обучения модели.

#### 4. Загрузка предобученной модели
Для дообучения использовалась предобученная модель sberbank-ai/rugpt3small_based_on_gpt2. Дообучение проводилось на мощностях Google Colab с использованием GPU Tesla A100, что позволило значительно ускорить процесс обучения и сократить время выполнения задач.



#### 5. Настройка параметров обучения
- Для обучения модели были заданы следующие параметры:
  - Количество эпох: 3.
  - Размер батча: 32.
  - Шаги накопления градиента: 8.
  - Использование смешанной точности (`fp16`) для ускорения обучения.
  - Сохранение модели каждые 500 шагов.
  - Логирование каждые 100 шагов.
  - Ограничение на количество сохраняемых чекпоинтов: 2.

#### 6. Обучение модели
- Обучение модели проводилось с использованием класса `Trainer` из библиотеки `transformers`.

[img](https://github.com/sultanovemil/PP_3_URFU/blob/main/img/learning.PNG)

---

### Заключение

Проект позволил создать модель, способную генерировать текстовые отзывы на основе заданных параметров. 




Веб-приложение развернуто на Hugging Face Spaces и доступно по следующей [ссылке](https://huggingface.co/spaces/Emil25/PP3_Team_1)
![app](https://github.com/sultanovemil/PP_3_URFU/blob/main/img/app.png)
